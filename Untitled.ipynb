{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3726fe7d-2821-4797-a245-a9378acbf762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoda\\A - Uni\\thesis\n",
      "Using random initialization for embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "__file__ = '../data/PEMS04.npz'\n",
    "file_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "print(file_dir)\n",
    "sys.path.append(file_dir)\n",
    "\n",
    "sys.argv=['']\n",
    "del sys\n",
    "#*************************************init_embedding************************************#\n",
    "import os\n",
    "import numpy as np\n",
    "from lib.dataloader import get_train_adj_matrix_and_embeddings\n",
    "\n",
    "# Configuration\n",
    "dataset_name = \"PEMSD4\"\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "prediction_length = 1  # Set prediction length\n",
    "lambda1 = 0.02\n",
    "embedding_dim = 10  # Set embedding dimensions\n",
    "\n",
    "# Flag to determine which initialization to use\n",
    "use_precomputed = False  # Set to False to use random initialization\n",
    "\n",
    "# File path for the precomputed embeddings\n",
    "E_init_file_name = f\"E_init/E_init_{dataset_name}_pre_len{prediction_length}.npy\"\n",
    "\n",
    "if use_precomputed:\n",
    "    if os.path.exists(E_init_file_name):\n",
    "        # If the file exists, load the precomputed embeddings\n",
    "        precomputed_embeddings = np.load(E_init_file_name)\n",
    "        print(f\"Loaded precomputed embeddings from {E_init_file_name}\")\n",
    "    else:\n",
    "        # Otherwise, generate the adjacency matrix and embeddings, then save them\n",
    "        _, precomputed_embeddings = get_train_adj_matrix_and_embeddings(\n",
    "            dataset_name, val_ratio, test_ratio, prediction_length, lambda1, embedding_dim)\n",
    "        np.save(E_init_file_name, precomputed_embeddings)\n",
    "        print(f\"Saved precomputed embeddings to {E_init_file_name}\")\n",
    "else:\n",
    "    precomputed_embeddings = None\n",
    "    print(\"Using random initialization for embeddings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74439e8c-4013-4847-b19d-de8b8acfc9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "node_embeddings torch.Size([307, 10]) True\n",
      "memory_weight torch.Size([]) True\n",
      "memory.Memory torch.Size([20, 64]) True\n",
      "memory.Wq torch.Size([64, 64]) True\n",
      "memory_proj.weight torch.Size([64, 64]) True\n",
      "memory_proj.bias torch.Size([64]) True\n",
      "encoder1.dcrnn_cells.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True\n",
      "encoder1.dcrnn_cells.0.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder1.dcrnn_cells.0.update.weights_pool torch.Size([10, 2, 65, 64]) True\n",
      "encoder1.dcrnn_cells.0.update.bias_pool torch.Size([10, 64]) True\n",
      "encoder1.dcrnn_cells.1.gate.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "encoder1.dcrnn_cells.1.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder1.dcrnn_cells.1.update.weights_pool torch.Size([10, 2, 128, 64]) True\n",
      "encoder1.dcrnn_cells.1.update.bias_pool torch.Size([10, 64]) True\n",
      "encoder2.dcrnn_cells.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True\n",
      "encoder2.dcrnn_cells.0.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder2.dcrnn_cells.0.update.weights_pool torch.Size([10, 2, 65, 64]) True\n",
      "encoder2.dcrnn_cells.0.update.bias_pool torch.Size([10, 64]) True\n",
      "encoder2.dcrnn_cells.1.gate.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "encoder2.dcrnn_cells.1.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder2.dcrnn_cells.1.update.weights_pool torch.Size([10, 2, 128, 64]) True\n",
      "encoder2.dcrnn_cells.1.update.bias_pool torch.Size([10, 64]) True\n",
      "end_conv1.weight torch.Size([12, 1, 1, 64]) True\n",
      "end_conv1.bias torch.Size([12]) True\n",
      "end_conv2.weight torch.Size([12, 64, 1, 12]) True\n",
      "end_conv2.bias torch.Size([12]) True\n",
      "end_conv3.weight torch.Size([12, 1, 1, 64]) True\n",
      "end_conv3.bias torch.Size([12]) True\n",
      "Total params num: 1513315\n",
      "*****************Finish Parameter****************\n",
      "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
      "Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)\n",
      "Val:    (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
      "Test:   (3375, 12, 307, 1) (3375, 12, 307, 1)\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "#from model.SGCRN import SGCRN_RD as Network\n",
    "from model.MegaCRN_RD import SGCRN_RD as Network\n",
    "from model.BasicTrainer import Trainer\n",
    "from lib.TrainInits import init_seed\n",
    "from lib.dataloader import get_dataloader\n",
    "from lib.TrainInits import print_model_parameters\n",
    "from lib.metrics import MAE_torch\n",
    "#*************************************************************************#\n",
    "#get configuration\n",
    "Mode = 'Train'\n",
    "DEBUG = 'True'\n",
    "DATASET = 'PEMSD4'      #PEMSD4 or PEMSD8\n",
    "DEVICE = 'cuda:0'\n",
    "MODEL = 'SGCRN'\n",
    "config_file = 'model/PEMSD4_SGCRN.conf'  # Assuming the file is in the same directory as your script\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "\n",
    "from lib.metrics import MAE_torch\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "#parser\n",
    "args = argparse.ArgumentParser(description='arguments')\n",
    "args.add_argument('--dataset', default=DATASET, type=str)\n",
    "args.add_argument('--mode', default=Mode, type=str)\n",
    "args.add_argument('--device', default=DEVICE, type=str, help='indices of GPUs')\n",
    "args.add_argument('--debug', default=DEBUG, type=eval)\n",
    "args.add_argument('--model', default=MODEL, type=str)\n",
    "args.add_argument('--cuda', default=True, type=bool)\n",
    "#data\n",
    "args.add_argument('--val_ratio', default=config['data']['val_ratio'], type=float)\n",
    "args.add_argument('--test_ratio', default=config['data']['test_ratio'], type=float)\n",
    "args.add_argument('--lag', default=config['data']['lag'], type=int)\n",
    "args.add_argument('--horizon', default=config['data']['horizon'], type=int)\n",
    "args.add_argument('--num_nodes', default=config['data']['num_nodes'], type=int)\n",
    "args.add_argument('--tod', default=config['data']['tod'], type=eval)\n",
    "args.add_argument('--normalizer', default=config['data']['normalizer'], type=str)\n",
    "args.add_argument('--column_wise', default=config['data']['column_wise'], type=eval)\n",
    "args.add_argument('--default_graph', default=config['data']['default_graph'], type=eval)\n",
    "#model\n",
    "args.add_argument('--input_dim', default=config['model']['input_dim'], type=int)\n",
    "args.add_argument('--output_dim', default=config['model']['output_dim'], type=int)\n",
    "args.add_argument('--embed_dim', default=config['model']['embed_dim'], type=int)\n",
    "args.add_argument('--rnn_units', default=config['model']['rnn_units'], type=int)\n",
    "args.add_argument('--num_layers', default=config['model']['num_layers'], type=int)\n",
    "args.add_argument('--cheb_k', default=config['model']['cheb_order'], type=int)\n",
    "#train\n",
    "args.add_argument('--loss_func', default=config['train']['loss_func'], type=str)\n",
    "args.add_argument('--seed', default=config['train']['seed'], type=int)\n",
    "args.add_argument('--batch_size', default=config['train']['batch_size'], type=int)\n",
    "args.add_argument('--epochs', default=config['train']['epochs'], type=int)\n",
    "args.add_argument('--lr_init', default=config['train']['lr_init'], type=float)\n",
    "args.add_argument('--lr_decay', default=config['train']['lr_decay'], type=eval)\n",
    "args.add_argument('--lr_decay_rate', default=config['train']['lr_decay_rate'], type=float)\n",
    "args.add_argument('--lr_decay_step', default=config['train']['lr_decay_step'], type=str)\n",
    "args.add_argument('--early_stop', default=config['train']['early_stop'], type=eval)\n",
    "args.add_argument('--early_stop_patience', default=config['train']['early_stop_patience'], type=int)\n",
    "args.add_argument('--grad_norm', default=config['train']['grad_norm'], type=eval)\n",
    "args.add_argument('--max_grad_norm', default=config['train']['max_grad_norm'], type=int)\n",
    "args.add_argument('--teacher_forcing', default=False, type=bool)\n",
    "#args.add_argument('--tf_decay_steps', default=2000, type=int, help='teacher forcing decay steps')\n",
    "args.add_argument('--real_value', default=config['train']['real_value'], type=eval, help = 'use real value for loss calculation')\n",
    "#test\n",
    "args.add_argument('--mae_thresh', default=config['test']['mae_thresh'], type=eval)\n",
    "args.add_argument('--mape_thresh', default=config['test']['mape_thresh'], type=float)\n",
    "#log\n",
    "args.add_argument('--log_dir', default='./', type=str)\n",
    "args.add_argument('--log_step', default=config['log']['log_step'], type=int)\n",
    "args.add_argument('--plot', default=config['log']['plot'], type=eval)\n",
    "args = args.parse_args()\n",
    "init_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(int(args.device[5]))\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n",
    "#init model\n",
    "# Initialize the model with precomputed embeddings\n",
    "model = Network(args, precomputed_embeddings=precomputed_embeddings)\n",
    "model = model.to(args.device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "print_model_parameters(model, only_num=False)\n",
    "\n",
    "#load dataset\n",
    "train_loader, val_loader, test_loader, scaler = get_dataloader(args,\n",
    "                                                               normalizer=args.normalizer,\n",
    "                                                               tod=args.tod, dow=False,\n",
    "                                                               weather=False, single=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ae0e4f-ef1f-4047-b24c-ee1273eb0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################Trainer##################################\n",
    "\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "args.log_dir = log_dir\n",
    "\n",
    "# # GPU\n",
    "# if torch.cuda.is_available():\n",
    "#     args.device = 'cuda:0'\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Switching to CPU.\")\n",
    "#     args.device = 'cpu'\n",
    "\n",
    "# model = model.to(args.device)\n",
    "\n",
    "# #start training\n",
    "# trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "#                   args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "# if args.mode.lower() == 'train':\n",
    "#     trainer.train()\n",
    "# elif args.mode.lower() == 'test':\n",
    "#     # Load the model on CPU\n",
    "#     model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "#     model.to(torch.device('cpu'))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n",
    "    \n",
    "#CPU\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "if args.mode.lower() == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    # Load the model on CPU\n",
    "    model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "    model.to(torch.device('cpu'))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd789524-0ce2-42cf-aeb5-290b89d35113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
